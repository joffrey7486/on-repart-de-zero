import numpy as np

# === 1. Donn√©es d'entr√©e (logique AND) ===
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

# Sorties attendues (truth)
T = np.array([0, 0, 0, 1])

# === 2. Initialisation ===
w = np.random.rand(2)   # poids al√©atoires pour x1 et x2
b = 0.0                 # biais initial
eta = 0.1               # taux d'apprentissage

# === 3. Fonction d'activation (Step Function) ===
def activation(z):
    return 1 if z > 0 else 0

# === 4. Entra√Ænement ===
for epoch in range(10):  # on r√©p√®te 10 fois le jeu d'entra√Ænement
    print(f"\nüîÅ √âPOQUE {epoch+1}")
    for i in range(len(X)):
        x = X[i]           # exemple courant
        t = T[i]           # sortie attendue
        
        # ---- √âtape 1 : Calcul du neurone ----
        z = np.dot(w, x) + b  # somme pond√©r√©e z = Œ£(w*x) + b
        y = activation(z)     # sortie pr√©dite apr√®s activation
        
        # ---- √âtape 2 : Calcul de l'erreur ----
        erreur = t - y
        
        # ---- Affichage AVANT correction ----
        print(f"\n   ‚ñ∂ AVANT correction : Entr√©e {x} | Sortie pr√©dite {y} | Attendue {t} | Poids {w} | Biais {b}")

        # ---- √âtape 3 : Mise √† jour des poids (Perceptron Rule) ----
        # w_new = w_old + Œ∑ * (t - y) * x
        w = w + eta * erreur * x
        # b_new = b_old + Œ∑ * (t - y)
        b = b + eta * erreur

        # ---- Affichage APR√àS correction ----
        print(f"   ‚úÖ APR√àS correction : Nouveau poids {w} | Nouveau biais {b} | Erreur appliqu√©e {erreur}")

# === 5. Test final du perceptron ===
print("\nüöÄ TEST FINAL APR√àS APPRENTISSAGE :")
for x in X:
    z = np.dot(w, x) + b
    y = activation(z)
    print(f"   {x} ‚Üí Sortie pr√©dite {y}")
